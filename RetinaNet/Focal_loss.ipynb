{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review : Focal Loss for Dense Object Detection\n",
    "========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "### Two-stage object detectors\n",
    "\n",
    "-\tCurrent state-of-the-art object detectors (proposal-driven mechanism)\n",
    "\n",
    "-\tR-CNN, FPN(Feature Pyramid Network)\n",
    "\n",
    "-\t**First stage**\n",
    "\n",
    "\t-\tgenerate set of candidate locations (region proposal)\n",
    "\t-\tSelective search, EdgeBoxes, DeepMask, RPN\n",
    "\n",
    "-\t**Second stage**\n",
    "\n",
    "\t-\tclassifies each candidate locations as one of the foreground classes or background using CNN\n",
    "\n",
    "-\tAddress class imbalance\n",
    "\n",
    "\t-\tFirst stage : reduce the number of candidate locations filtering out most background samples\n",
    "\t-\tSecond stage : maintain balance between foreground and background\n",
    "\t-\tfixed foreground-to-background ratio\n",
    "\t-\tonline hard example mining(OHEM)\n",
    "\n",
    "### One-stage object detectors\n",
    "\n",
    "-\tYOLO, SSD\n",
    "\n",
    "-\tFaster but lower accuracy than two-stage object detectors\n",
    "\n",
    "-\t**extreme foreground-background class imbalance**\n",
    "\n",
    "\t-\tlarger set of candidate object locations regularly sampled across an inage\n",
    "\n",
    "### New loss function : Focal loss\n",
    "\n",
    "![Figure1](https://user-images.githubusercontent.com/56924420/135286640-375c8b11-41f3-4804-8890-4461e93f7dff.PNG)\n",
    "\n",
    "-\tDesigned to address extreme foreground and background class imbalance in one-stage object detectors\n",
    "\n",
    "-\tScaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases\n",
    "\n",
    "\t-\tAutomatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples\n",
    "\n",
    "### RetinaNet\n",
    "\n",
    "-\tSimple one-stage object detector to demonstrate the effectiveness of the focal Loss\n",
    "\n",
    "-\tBased on ResNet-101-FPN backbone and use anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Related Work\n",
    "------------\n",
    "\n",
    "### Classic Object Detectors\n",
    "\n",
    "-\tBased sliding-window\n",
    "\n",
    "-\tHOG(Histogram of gradient), DPM(Deformable part model)\n",
    "\n",
    "### Two-stage detectors\n",
    "\n",
    "-\tSelective search\n",
    "\n",
    "\t-\tFirst stage generate a sparse seet of candidate proposals that should contain all objects while filtering out the majority of negative locations\n",
    "\t-\tSecond stage classifies the proposals into foreground classes / background\n",
    "\n",
    "-\tR-CNN\n",
    "\n",
    "\t-\tUpgrade the classifier to a convolutional network\n",
    "\n",
    "-\tFaster R-CNN\n",
    "\n",
    "\t-\tUse RPN(Region Proposal Networks)\n",
    "\n",
    "### One-stage detectors\n",
    "\n",
    "-\tSSD (Single Shot multibox Detector)\n",
    "\n",
    "\t-\t10 ~ 20% lower AP than two-stage methods\n",
    "\n",
    "-\tYOLO (You Only Look Once)\n",
    "\n",
    "\t-\tFocus on an even more extreme speed/accuracy trade-off\n",
    "\n",
    "### RetinaNet\n",
    "\n",
    "-\tShares many similarites with previous detectors\n",
    "\n",
    "\t-\t'Anchors' introduced RPN\n",
    "\t-\tFeature pyramid in SSD and FPN\n",
    "\n",
    "-\t**Its achieves top results not based on innovations in network design but due to focal loss**\n",
    "\n",
    "### Class imbalance\n",
    "\n",
    "-\tClassic(DPMs) and recent(SSD) methods face a large class imbalance during training\n",
    "\n",
    "-\tEvaluate 10k~100k candidate locations per image but only a few locations contain objects\n",
    "\n",
    "-\tTwo problems\n",
    "\n",
    "\t1.\tTraining is inefficient as most locations are easy negative that contribute no useful learning signal\n",
    "\t2.\tEasy negative can overwhelm training and lead to degenerate models\n",
    "\n",
    "-\tA common solution is hard negative mining\n",
    "\n",
    "\t-\tSamples hard examples during training\n",
    "\n",
    "-\tIn contrast, focal loss naturally handles the class imbalance without sampling and without easy negatives overwhelming the loss and computed gradients\n",
    "\n",
    "### Robust estimation\n",
    "\n",
    "-\tDesigning robust loss functions(Huber loss) that reduce the contribution of outliers by down-weighting the loss of examples with large errors(hard examples)\n",
    "\n",
    "-\tIn contrast, focal loss that reduce the contribution of inliers by down-weighting the loss of examples with small errors(easy examples)\n",
    "\n",
    "-\tFocal loss and robust loss have opposite roles\n",
    "\n",
    "\t-\tFocal loss focuses training on a sparse set of hard examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Focal Loss\n",
    "----------\n",
    "\n",
    "### Binary Cross Entropy\n",
    "\n",
    "$$ \\text{CE}(p,y) = \\begin{cases} -\\text{log}(p) & \\text{if} \\; y = 1 \\\\ -\\text{log}(1-p) & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "-\t$y \\in \\{ \\pm 1\\}$ specifies the ground-truth class\n",
    "\n",
    "-\t$p \\in [0,1]$ is the model's estimated probability for the class with label $y=1$\n",
    "\n",
    "$$ p_t = \\begin{cases} p & \\text{if} \\; y = 1 \\\\ 1-p & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "-\tHence $\\text{CE}(p,y) = \\text{CE}(p_t) = - \\text{log}(p_t)$\n",
    "\n",
    "-\tOne notable property of CE is that even examples that are easily classified ($p_t \\gg 0.5$) incur a loss with non-trivial magnitude\n",
    "\n",
    "-\tWhen summed over a large number of easy examples, these small loss values can overwhelm the rare class\n",
    "\n",
    "### Balanced Cross Entropy\n",
    "\n",
    "$$ \\text{CE}(p_t) = - \\alpha_t \\text{log}(p_t)$$\n",
    "\n",
    "-\t$\\alpha$ is a weighting factor\n",
    "\t-\t$\\alpha \\in [0,1]$ for class 1, and $1 - \\alpha$ for class -1\n",
    "\n",
    "![BCE_loss_plot](https://user-images.githubusercontent.com/56924420/135324067-cb681504-8e92-4140-b10b-da0cb3aaec6d.png)\n",
    "\n",
    "### Focal Loss Definition\n",
    "\n",
    "-\tWhile $\\alpha$ balances the importance of positive/negative examples, it does not differentiate between easy/hard examples\n",
    "\n",
    "-\tReshape the loss function to down-weight easy examples and thus focus training on hard negatives\n",
    "\n",
    "$$ \\text{FL}(p_t) = -(1- p_t)^\\gamma \\text{log}(p_t)$$\n",
    "\n",
    "-\t$(1-p_t)^\\gamma$ is a modulating factor, $\\gamma \\ge 0$ is tuneable parameter\n",
    "\n",
    "-\tTwo properties of the focal loss\n",
    "\n",
    "\t1.\tWhen an example is misclassified and $p_t$ is small, the modulating factor is near 1 and loss is unaffected $p_t \\rightarrow 1$, factor goes to 0 and the loss for well-classified exampls is down-weighted\n",
    "\n",
    "\t2.\tWhen $\\gamma$ is increased, the effect of the modulation factor is increased\n",
    "\n",
    "-\tThe modulating factor reduces the loss contribution from easy examples\n",
    "\n",
    "$$ \\text{FL}(p_t) = -{\\alpha_t}(1- p_t)^\\gamma \\text{log}(p_t)$$\n",
    "\n",
    "-\t$\\alpha$-balanced variant of the focal loss\n",
    "\n",
    "### Class imbalance and Model Initialization\n",
    "\n",
    "-\tBinary classification models are by default initialized to have equal probability of outputting either $y = -1 \\text{or} \\, 1$\n",
    "\n",
    "-\tWe introduce the concept of a 'prior' for the value of $p$ estimated by the model for the rare class at the start or training\n",
    "\n",
    "-\tDenote the prior by $\\pi$\n",
    "\n",
    "-\tThis improve training stability for both the cross entropy and focal loss in the case of heavy class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "RetinaNet Detector\n",
    "------------------\n",
    "\n",
    "![Figure3](https://user-images.githubusercontent.com/56924420/135328928-ebf3ed2b-54af-4054-99be-ad193461c931.PNG)\n",
    "\n",
    "-\tRetinaNet is a single, unified network composed of a **backbone network** and **two task-specific subnetworks**\n",
    "\n",
    "### Feature Pyramid Network Backbone\n",
    "\n",
    "-\tFPN create multi-scale feature pyramid from a single resolution input image\n",
    "\n",
    "-\tEach level of the pyramid can be used for detecting objects at a different scale\n",
    "\n",
    "-\tFPN improves multi-scale predictions from fully convolutional networks\n",
    "\n",
    "-\tBuild FPN on top of the ResNet architecture\n",
    "\n",
    "-\tpyramid with levels $P_3$ through $P_7$, all pyramid levels have $C = 256$ channels\n",
    "\n",
    "### Anchors\n",
    "\n",
    "-\tEach pyramid level use anchors at three aspect ratio {$1:2, 1:1, 2:1$}\n",
    "-\tEach pyramid level add anchors of sizes {$2^0, 2^{1/3}, 2^{2/3}$} of the original set of 3 aspect ratio anchors\n",
    "-\tTotal $A = 9$ anchors per level and across levels they cover the scale range 32-813 pixels\n",
    "-\tAnchors are assigned to ground-truth object boxes using an IoU threshold of 0.5 and to background if their IoU is in [0, 0.4)\n",
    "-\tIf IoU is in [0.4, 0.5), it is ignored during training\n",
    "\n",
    "### Classification Subnet\n",
    "\n",
    "-\tPredicts the probability of object presence at each spatial position for each of the $A$ anchors and $K$ object classes\n",
    "-\tThis subnet is a small FCN attached to each FPN level\n",
    "-\tApplies four $3 \\times 3$ conv layers, each with $C$ filters and each followed ReLU activation\n",
    "\n",
    "### Box Regression Subnet\n",
    "\n",
    "-\tIn parallel with the object classification subnet, we attach another small FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object\n",
    "-\t4 offset are (x_center, y_center, width, height)\n",
    "-\tUse class-agnostic bounding box regressor\n",
    "\t-\tUses fewer parameter and effective\n",
    "\n",
    "Inference and Training\n",
    "----------------------\n",
    "\n",
    "### Inference\n",
    "\n",
    "-\tTo improve speed, we only decode box predictions from at most 1k top-scoreing predictions per FPN level, after thresholding detector confidence at 0.05\n",
    "-\tFinal detections, use non-maximum suppression with a threshold of 0.5\n",
    "\n",
    "### Focal Loss\n",
    "\n",
    "-\tUse the loss on the output of the classification subnet\n",
    "-\tFocal loss is applied to all ~100k anchors in each sampled image and is computed as the sum of all anchors\n",
    "-\tNote that $\\alpha$, the weight assigned to the rare class, also has a stable range, but it interacts with $\\gamma$ making it necessary to select the two together\n",
    "\n",
    "-\t$\\gamma = 2,\\, \\alpha = 0.25$ work best\n",
    "\n",
    "### Initialization\n",
    "\n",
    "-\tExperiment with ResNet-50-FPN and ResNet-101-FPN backbone\n",
    "-\tModels are pre-trained on ImageNet1K\n",
    "-\tAll new conv layers except the final one in the RetinaNet subnets are initialized with bias $b = 0$ and a Gaussian weight fill with $\\sigma = 0.01$\n",
    "-\tFor the final conv layer of the classification subnet, set the bias Initialization to $ b = -\\text{log}((1- \\pi)/\\pi),\\, \\pi = 0.01$\n",
    "\n",
    "### Optimization\n",
    "\n",
    "-\tTrained with stochastic gradient descent(SGD) over 8 GPUs with a total of 16 images per minibatch\n",
    "-\tInitial learning rate = 0.01, total 90k epochs\n",
    "-\tat 60k and 80k, learning rate = 0.001, 0.0001 (divided by 10)\n",
    "-\tweight decay = 0.001, momentum = 0.9\n",
    "-\tclass predict : focal loss / box regression : standard smooth $L_1$ loss\n",
    "-\tTraining time ranges between 10 and 35 hours\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Experiments\n",
    "---------------\n",
    "\n",
    "## Training Dense Detection\n",
    "\n",
    "### Network Initialization\n",
    "\n",
    "-\tFirst uses standart cross entropy(CE) loss but fails quickly with the network diverging during training\n",
    "-\tSimply initializing the last layer of our model such that the prior probability of detecing an object is $\\pi = 0.01$ enables effective learning\n",
    "\t-\tAP 30.2 on COCO\n",
    "-\tso we use $\\pi = 0.01$ for all experiments\n",
    "\n",
    "### Balanced Cross Entropy\n",
    "\n",
    "-\tNext uses $\\alpha$-balanced CE loss\n",
    "\n",
    "![Table1-(a)](https://user-images.githubusercontent.com/56924420/135340066-ffd074ce-463b-49fe-ae51-62d60f8cd220.PNG)\n",
    "\n",
    "-\t$\\alpha = 0.75$ gives of 0.9 points AP\n",
    "\n",
    "### Focal Loss\n",
    "\n",
    "![Table1-(b)](https://user-images.githubusercontent.com/56924420/135341584-fe4c3d64-e775-4772-bb3e-53c99d5b02ac.PNG)\n",
    "\n",
    "-\tFor a fair comparison we find the best $\\alpha$ for each $\\gamma$\n",
    "-\tThe benefit of chaninging $\\gamma$ is much larger, and indeed the best $\\alpha's$ ranged in just [0.25, 0.75]\n",
    "-\tWe use $\\gamma= 2.0 \\, \\text{with} \\, \\alpha = 0.25$, but $\\alpha = 0.4$ works nearly as well\n",
    "\n",
    "### Analysis of the Focal Loss\n",
    "\n",
    "-\tTo understand the focal loss better, we analyze the emprical dirtribution of the loss of a converged model\n",
    "\n",
    "-\tWe take our default ResNet-101 600-pixel model trained with $\\gamma = 2$\n",
    "\n",
    "![Figure4](https://user-images.githubusercontent.com/56924420/135345787-53026669-7680-4eec-920a-9f36dac149a3.PNG)\n",
    "\n",
    "-\tCumulative distribution functions for positive and negative samples\n",
    "-\tObserve the positive samples, we see that the CDF looks fairly similar for different values of $\\gamma$\n",
    "-\tThe effect of $\\gamma$ on negative samples is dramatically different\n",
    "-\tFL can effectively discount the effect of easy negatives, focusing all attention on the hard negative examples\n",
    "\n",
    "### Online Hard Example Mining(OHEM)\n",
    "\n",
    "-\tLike the focal loss, OHEM puts more emphasis on misclassified example, but completely discards easy examples\n",
    "\n",
    "![Table1-(d)](https://user-images.githubusercontent.com/56924420/135347622-13fec800-c0fa-4183-ba50-94caa3f0bb93.PNG)\n",
    "\n",
    "Model Architecture Design\n",
    "-------------------------\n",
    "\n",
    "### Anchor Density\n",
    "\n",
    "-\tOne-stage detectors use a fixed sampling grid, use multiple 'anchors' at each spatial position to cover boxes of various scales and aspect ratios\n",
    "\n",
    "![Table1-(c)](https://user-images.githubusercontent.com/56924420/135349056-15a10520-b2b4-4d72-b93c-43e6c810723f.PNG)\n",
    "\n",
    "### Speed versus Accuracy\n",
    "\n",
    "![Figure2](https://user-images.githubusercontent.com/56924420/135349250-97ca9ec5-3284-4ee1-8061-11930d111ab5.PNG)\n",
    "\n",
    "![Table1-(e)](https://user-images.githubusercontent.com/56924420/135349180-b037bbb8-14c6-4d30-bac6-757474776cd4.PNG)\n",
    "\n",
    "### Comparison to State of the Art\n",
    "\n",
    "![Table2](https://user-images.githubusercontent.com/56924420/135349678-80dbd607-ea73-43eb-867d-db5a982e0643.PNG)\n",
    "\n",
    "---\n",
    "\n",
    "Conclusion\n",
    "----------\n",
    "\n",
    "-\tWe identify class imbalance as the primary obstacle preventing one-stage object detectors from surpassing top-perfoming, two-stage methods\n",
    "-\tFocal loss which applies a modulating term to the croee entropy loss in order to focus learning on hard negative examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
